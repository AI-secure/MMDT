# MMDT: Decoding the Trustworthiness and Safety of Multimodal Foundation Models

[[Website ğŸŒ]](https://mmdecodingtrust.github.io/), [[Text-to-Image data ğŸ¤—]](https://huggingface.co/datasets/AI-Secure/MMDecodingTrust-T2I), [[Image-to-Text data ğŸ¤—]](https://huggingface.co/datasets/AI-Secure/MMDecodingTrust-I2T)

## Overview

This repo contains the source code of MMDT (Multimodal DecodingTrust). This research endeavor is designed to help researchers and practitioners better understand the capabilities, limitations, and potential risks involved in deploying these state-of-the-art Multimodal foundation models (MMFMs). See our paper for details.


This project is organized around the following six primary perspectives of trustworthiness, including:
1. Safety
2. Hallucination
3. Fairness
4. Privacy
5. Adversarial robustness
6. Out-of-Distribution Robustness

## Project Structure
This project is structured around subdirectories dedicated to each area of trustworthiness. Each subdir includes scripts, data, and a dedicated README for easy comprehension.


## Getting Started

### Clone the repository

```bash
git clone https://github.com/AI-secure/MMDT.git && cd MMDT
```

### Install requirements

Create a new environment:

```bash
conda create --name mmdt python=3.9
conda activate mmdt
```

Install PyTorch following [this link](https://pytorch.org/get-started/locally/). Then install the requirements:

```bash
pip install -r requirements.txt
python -m spacy download en_core_web_sm
```

### Evaluate all perspectives

```bash
bash scripts/t2i.sh {model_id}  # Evaluate a text-to-image model
bash scripts/i2t.sh {model_id}  # Evaluate an image-to-text model
```

### Evaluate each perspective

We also provide off-the-shelf scripts for evaluating each perspective under `./scripts`.
For example, the following script evaluates all scenarios and tasks of image-to-text modality for the hallucination perspective.
```bash
bash scripts/hallucination_i2t.sh gpt-4o
```
An example of the output summarized score can be found [here](./mmdt/perspectives/hallucination/README.md?plain=1#L76).

Moreover, you can customize the evaluation process with specific perspective, scenario, and task by running the following script:
```
python mmdt/main.py --modality {modality} --model_id {model_id} --perspectives {perspective} --scenario {scenario} --task {task}
```

For example, to evaluate `gpt-4o` on hallucination under `natural selection` scenario and `action recognition` task, we can run the following example script.
```bash
python mmdt/main.py --modality image_to_text --model_id gpt-4o --perspectives hallucination --scenario natural --task action
```

Our framework includes the following perspectives, scenarios, and tasks:
```
Text-to-image models
â”œâ”€â”€ safety
â”‚   â”œâ”€â”€ vanilla
â”‚   â”œâ”€â”€ transformed
â”‚   â””â”€â”€ jailbreak
â”œâ”€â”€ hallucination
â”‚   â”œâ”€â”€ natural
â”‚   â”‚   â”œâ”€â”€ identification
â”‚   â”‚   â”œâ”€â”€ attribute
â”‚   â”‚   â”œâ”€â”€ spatial
â”‚   â”‚   â””â”€â”€ count
â”‚   â”œâ”€â”€ counterfactual
â”‚   â”‚   â”œâ”€â”€ identification
â”‚   â”‚   â”œâ”€â”€ attribute
â”‚   â”‚   â”œâ”€â”€ spatial
â”‚   â”‚   â””â”€â”€ count
â”‚   â”œâ”€â”€ misleading
â”‚   â”‚   â”œâ”€â”€ identification
â”‚   â”‚   â”œâ”€â”€ attribute
â”‚   â”‚   â”œâ”€â”€ spatial
â”‚   â”‚   â””â”€â”€ count
â”‚   â”œâ”€â”€ distraction
â”‚   â”‚   â”œâ”€â”€ identification
â”‚   â”‚   â”œâ”€â”€ attribute
â”‚   â”‚   â”œâ”€â”€ spatial
â”‚   â”‚   â””â”€â”€ count
â”‚   â”œâ”€â”€ ocr
â”‚   â”‚   â”œâ”€â”€ complex
â”‚   â”‚   â”œâ”€â”€ contradictory
â”‚   â”‚   â”œâ”€â”€ distortion
â”‚   â”‚   â””â”€â”€ misleading
â”‚   â””â”€â”€ cooccurrence
â”‚       â”œâ”€â”€ identification
â”‚       â”œâ”€â”€ attribute
â”‚       â”œâ”€â”€ spatial
â”‚       â””â”€â”€ count
â”œâ”€â”€ fairness
â”‚   â”œâ”€â”€ stereotype
â”‚   â”œâ”€â”€ decision_making
â”‚   â”œâ”€â”€ overkill
â”‚   â””â”€â”€ individual
â”œâ”€â”€ privacy
â”‚   â””â”€â”€ training
â”‚       â””â”€â”€ laion_1k
â”œâ”€â”€ adv
â”‚   â””â”€â”€ adv
â”‚       â”œâ”€â”€ object
â”‚       â”œâ”€â”€ attribute
â”‚       â””â”€â”€ spatial
â””â”€â”€ ood
    â”œâ”€â”€ Shake_
    â”‚   â”œâ”€â”€ helpfulness
    â”‚   â”œâ”€â”€ count
    â”‚   â”œâ”€â”€ spatial
    â”‚   â”œâ”€â”€ color
    â”‚   â””â”€â”€ size
    â””â”€â”€ Paraphrase_
        â”œâ”€â”€ helpfulness
        â”œâ”€â”€ count
        â”œâ”€â”€ spatial
        â”œâ”€â”€ color
        â””â”€â”€ size

Image-to-text models
â”œâ”€â”€ safety
â”‚   â”œâ”€â”€ typography
â”‚   â”œâ”€â”€ illustration
â”‚   â””â”€â”€ jailbreak
â”œâ”€â”€ hallucination
â”‚   â”œâ”€â”€ natural
â”‚   â”‚   â”œâ”€â”€ identification
â”‚   â”‚   â”œâ”€â”€ attribute
â”‚   â”‚   â”œâ”€â”€ spatial
â”‚   â”‚   â”œâ”€â”€ count
â”‚   â”‚   â””â”€â”€ action
â”‚   â”œâ”€â”€ counterfactual
â”‚   â”‚   â”œâ”€â”€ identification
â”‚   â”‚   â”œâ”€â”€ attribute
â”‚   â”‚   â”œâ”€â”€ spatial
â”‚   â”‚   â””â”€â”€ count
â”‚   â”œâ”€â”€ misleading
â”‚   â”‚   â”œâ”€â”€ identification
â”‚   â”‚   â”œâ”€â”€ attribute
â”‚   â”‚   â”œâ”€â”€ spatial
â”‚   â”‚   â”œâ”€â”€ count
â”‚   â”‚   â””â”€â”€ action
â”‚   â”œâ”€â”€ distraction
â”‚   â”‚   â”œâ”€â”€ identification
â”‚   â”‚   â”œâ”€â”€ attribute
â”‚   â”‚   â”œâ”€â”€ spatial
â”‚   â”‚   â”œâ”€â”€ count
â”‚   â”‚   â””â”€â”€ action
â”‚   â”œâ”€â”€ ocr
â”‚   â”‚   â”œâ”€â”€ contradictory
â”‚   â”‚   â”œâ”€â”€ cooccur
â”‚   â”‚   â”œâ”€â”€ doc
â”‚   â”‚   â””â”€â”€ scene
â”‚   â””â”€â”€ cooccurrence
â”‚       â”œâ”€â”€ identification
â”‚       â”œâ”€â”€ attribute
â”‚       â”œâ”€â”€ spatial
â”‚       â”œâ”€â”€ count
â”‚       â””â”€â”€ action
â”œâ”€â”€ fairness
â”‚   â”œâ”€â”€ stereotype
â”‚   â”œâ”€â”€ decision_making
â”‚   â”œâ”€â”€ overkill
â”‚   â””â”€â”€ individual
â”œâ”€â”€ privacy
â”‚   â”œâ”€â”€ location
â”‚   â”‚   â”œâ”€â”€ Pri-SV-with-text
â”‚   â”‚   â”œâ”€â”€ Pri-SV-without-text
â”‚   â”‚   â”œâ”€â”€ Pri-4Loc-SV-with-text
â”‚   â”‚   â””â”€â”€ Pri-4Loc-SV-without-text
â”‚   â””â”€â”€ pii
â”œâ”€â”€ adv
â”‚   â””â”€â”€ adv
â”‚       â”œâ”€â”€ object
â”‚       â”œâ”€â”€ attribute
â”‚       â””â”€â”€ spatial
â””â”€â”€ ood
    â”œâ”€â”€ Van_Gogh
    â”‚   â”œâ”€â”€ attribute
    â”‚   â”œâ”€â”€ count
    â”‚   â”œâ”€â”€ spatial
    â”‚   â””â”€â”€ identification
    â”œâ”€â”€ oil_painting
    â”‚   â”œâ”€â”€ attribute
    â”‚   â”œâ”€â”€ count
    â”‚   â”œâ”€â”€ spatial
    â”‚   â””â”€â”€ identification
    â”œâ”€â”€ watercolour_painting
    â”‚   â”œâ”€â”€ attribute
    â”‚   â”œâ”€â”€ count
    â”‚   â”œâ”€â”€ spatial
    â”‚   â””â”€â”€ identification
    â”œâ”€â”€ gaussian_noise
    â”‚   â”œâ”€â”€ attribute
    â”‚   â”œâ”€â”€ count
    â”‚   â”œâ”€â”€ spatial
    â”‚   â””â”€â”€ identification
    â”œâ”€â”€ zoom_blur
    â”‚   â”œâ”€â”€ attribute
    â”‚   â”œâ”€â”€ count
    â”‚   â”œâ”€â”€ spatial
    â”‚   â””â”€â”€ identification
    â””â”€â”€ pixelate
        â”œâ”€â”€ attribute
        â”œâ”€â”€ count
        â”œâ”€â”€ spatial
        â””â”€â”€ identification
```


### Notes
+ Each of the six perspectives has its subdirectory containing the respective code and README.

+ Follow the specific `README`: Every subdirectory has its own README. Refer to these documents for information on how to run the scripts and interpret the results.

## License
This project is licensed under the [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/legalcode)  - see the LICENSE file for details.

## Contact
Please reach out to us if you have any questions or suggestions. You can submit an issue or pull request, or send an email to chejian2@illinois.edu.

Thank you for your interest in MMDT. We hope our work will contribute to a more trustworthy, fair, and robust AI future.
